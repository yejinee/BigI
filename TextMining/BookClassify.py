# -*- coding: utf-8 -*-
"""project정리버전

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IFHhRr8qr1BTV7bQl1sUTU4wGL4qzG19
"""

## 필요모듈 임포트


import io
import os
import re
import pandas as pd
import matplotlib as plt
import seaborn as sns
import itertools
import nltk 
import string
from wordcloud import WordCloud
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob,Word
from collections import Counter

#visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns
import plotly 
import plotly.graph_objs as go
import plotly.figure_factory as ff
import plotly.io as pio
from plotly.subplots import make_subplots
import plotly.express as px
from plotly.offline import iplot, init_notebook_mode
import cufflinks as cf
import plotly.figure_factory as ff 
from plotly.offline import iplot
from plotly import tools
colors = px.colors.qualitative.Prism
pio.templates.default = "plotly_white"

import numpy as np
import pandas as pd
import io
from collections import Counter
from tensorflow.keras.preprocessing.text import text_to_word_sequence

from sklearn.preprocessing import LabelEncoder
import re
import nltk
from nltk.tokenize import sent_tokenize
from statistics import mean
nltk.download('punkt')

## 모델 학습
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor


## 모델 저장용
from sklearn import datasets
import pickle
from sklearn.externals import joblib

## data set 불러오기

from google.colab import files
trainfile=files.upload()

df_train = data=pd.read_csv(io.BytesIO(trainfile['train.csv']))

################################ target을 a,b,c 구간으로 분류

target_group = df_train.copy()

## 타겟 점수 구간 분류

target_group['target'] = pd.qcut(target_group.target, q= 3, labels = ['a','b','c'])

a_group = target_group.loc[target_group.target =='a', :]

b_group = target_group.loc[target_group.target =='b', :]

c_group = target_group.loc[target_group.target =='c', :]

## 구간 별 단어 리스트 제작
## a구간 : 발췌문 난이도 상
## b구간 : 발췌문 난이도 중
## c구간 : 발췌문 난이도 하

###### a구간 단어 리스트 
words_a=[]
for s in a_group.excerpt:
  words_a.extend(text_to_word_sequence(s))

## a구간 단어 별 사용된 빈도수 ##사용하진 않았음

cnt_a = Counter(words_a)

## a구간 단어 셋
set_a = list(set(words_a))

len(set_a)

###### b 구간 단어 리스트
words_b=[]
for s in b_group.excerpt:
  words_b.extend(text_to_word_sequence(s))

## b구간 단어 별 사용된 빈도수 ##사용하진 않았음
cnt_b = Counter(words_b)

## b구간 단어 셋

set_b = list(set(words_b))

###### c구간 단어 리스트
words_c=[]
for s in c_group.excerpt:
  words_c.extend(text_to_word_sequence(s))

## b구간 단어 별 사용된 빈도수 ##사용하진 않았음
cnt_c = Counter(words_c)

## c구간 단어 셋

set_c = list(set(words_c))

###### a구간에만 있는 단어 구하기

only_afirst = list(set(set_a) - set(set_b))

only_a = list(set(only_afirst) - set(set_c))

###### b구간에만 있는 단어 구하기
only_bfirst = list(set(set_b) - set(set_a))

only_b = list(set(only_bfirst) - set(set_c))

###### c구간에만 있는 단어 구하기(1차)
only_cfirst = list(set(set_c) - set(set_a))

only_c = list(set(only_cfirst) - set(set_b))

### a,b,c 구간 별 단어 수 
## a구간 단어 수: 8755
## b구간 단어 수: 5907
## c구간 단어 수: 3718

###### 구간 별, stopword제거

## a구간 stopword제거
from nltk.tokenize import word_tokenize  
a_stop = [] 
for a in only_a: # token에 있는 단어 불러오기
  if a not in stopwords.words('english'): # 불러온 단어가 stopwords에 있는가?
    a_stop.append(a)  # 없으면 result에 넣기

## b구간 stopword제거

b_stop = []
for word in only_b:   
  if word not in stopwords.words('english'):  
    b_stop.append(word)  

len(b_stop)

## b구간 stopword제거

c_stop = []
for word in only_c:   # token에 있는 단어 불러오기
  if word not in stopwords.words('english'):   # 불러온 단어가 stopwords에 있는가?
    c_stop.append(word)   # 없으면 result에 넣기

## stopword 제거 된 단어 리스트 -> DataFrame화

df_a_stop = pd.DataFrame(a_stop)
df_b_stop = pd.DataFrame(b_stop)
df_c_stop = pd.DataFrame(c_stop)

####### 구간 별 숫자 제거
####### 숫자를 먼저 제거하고, stopword를 제거하는 것이 더 나을 것 같아 나중에 수정 필요!
####### [0-9]를 xxxx로 변환해서, 숫자랑 글자랑 붙어 있는 경우를 해결하고자 했으나, 한시적인 해결책이므로 수정 필요


## a구간 숫자 제거

aaa = []
for i in range(len(df_a_stop)):
  a = re.sub('[0-9]','XXXX', df_a_stop.iloc[i,0])
  aaa.append(a)

## b구간 숫자 제거

bbb = []
for i in range(len(df_b_stop)):
  b = re.sub('[0-9]','XXXX', df_b_stop.iloc[i,0])
  bbb.append(b)

## c구간 숫자 제거

ccc = []
for i in range(len(df_c_stop)):
  c = re.sub('[0-9]','XXXX', df_c_stop.iloc[i,0])
  ccc.append(c)

## 숫자 제거된 단어 리스트 다시, DataFrame화 
## 한번에 [숫자, stopword, 겹치는 단어] 다 제거하고, DataFrame으로 바꾸는 것이 훨씬 나을것 같아 나중에 수정 필요

df_aa = pd.DataFrame(aaa)
df_bb = pd.DataFrame(bbb)
df_cc = pd.DataFrame(ccc)

## 구간 별 가중치 입력

df_aa['score'] = 3
df_bb['score'] = 2
df_cc['score'] = 1

## 가중치 입력된 단어들 한 DataFrame으로 합치기

df1 = pd.merge(df_aa, df_bb, how='outer', on=None)

df_word_score = pd.merge(df1, df_cc, how='outer', on=None)

## 칼럼명 수정
df_word_score.columns = ['words', 'score']

######### train_set의 단어들에게 가중치 부여를 위해, df_train.excerpt를 리스트 변환

wordslist = []
for i in df_train.excerpt:
  wordslist.append(text_to_word_sequence(i))

####### df_train.excerpt 단어 리스트와 df_score_words 리스트 비교 및 가중치 점수 부여

totalscore=[]
cnt = 0
for i in wordslist:
  score=0
  for j in i:
    if j in list(df_word_score.words):
      score+=df_word_score.loc[df_word_score.words == j, :].score.values[0]
  totalscore.append(score)

freq_excerpt_a=[]
for sw in splitword:
  cntless=0
  for j in sw:
    if j in word_ind:
      cntless+=1
  freq_excerpt_a.append(cntless)

################################## 문장 수, 단어 수, 가장 긴 문장 길이, 짧은 문장 길이 등의 feature 추가
## 특수기호 및 숫자 제거
to_replace_by_space = re.compile('[/(){}\[|]|@,;')
punctuation = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')
bad_symbols = re.compile('[^0-9a-z. #+_]')

def text_prepare(text):
  text = text.lower()
  #text = re.sub(punctuation, '', text)
  text = re.sub(to_replace_by_space, " ", text)
  text = re.sub(bad_symbols,"", text)
  text = re.sub(' +',' ', text)
  return text

## 특수기호 및 숫자 제거
df_train['excerpt'] = df_train['excerpt'].apply(text_prepare)                     
prepared_as_text = [line for line in df_train['excerpt']]
text_prepared_results = '/n'.join(prepared_as_text)
text = ' '.join(t for t in df_train['excerpt'])

## test용

'''df_test['excerpt'] = df_test['excerpt'].apply(text_prepare)                     
prepared_as_text = [line for line in df_test['excerpt']]
text_prepared_results = '/n'.join(prepared_as_text)
text = ' '.join(t for t in df_test['excerpt'])'''

# 단어를 추출해서 list에 넣기 
words=[]
for s in df_train.excerpt:
  words.extend(text_to_word_sequence(s))

## test용
'''from tensorflow.keras.preprocessing.text import text_to_word_sequence
# 단어를 추출해서 list에 넣기 
words_test=[]
for s in df_test.excerpt:
  words.extend(text_to_word_sequence(s))'''

# 1개 발췌문에 있는 문장의 갯수
cnt_sent = df_train.excerpt.str.split('.').apply(len)
df_train['cnt_sent'] = cnt_sent

## test용
cnt_sent_test = df_test.excerpt.str.split('.').apply(len)
df_test['cnt_sent'] = cnt_sent_test

def min_max_mean_sentence_length(text):

    tokened_sent = sent_tokenize(text)
    main_dict = {}
    for item in tokened_sent:
        item1 = list(item.split(" "))
        item2 = [' '.join(item1)]
        Length = []
        Length.append(len(item1))
        mydict = dict(zip(item2, Length))
        main_dict.update(mydict)

    return max(main_dict.values()), min(main_dict.values()), round(mean(main_dict.values()),3)

# column 추가하는 함수 (length, word cnt)
def basic_features(_):
    df= _.copy()
    df['excerpt_len'] = df['excerpt'].apply(lambda x : len(x))
    df['excerpt_word_count'] = df['excerpt'].apply(lambda x : len(x.split(' ')))
    df[['max_len_sent','min_len_sent','avg_len_sent']] = df.apply(lambda x: min_max_mean_sentence_length(x['excerpt']),axis=1, result_type='expand')
    return df

## 글자 길이, 단어 수, 가장 긴 문장의 길이, 짧은 문장의 길이, 평균 문장의 길이를 feature에 추가
df_train = basic_features(df_train)

## 테스트용
'''df_test = basic_features(df_test)'''

############### 많이 쓰지 않는 단어 확인
# word_ind 에 빈도수 1인 단어는 17115개
cnt_word = Counter(words) 
word_ind = cnt_word.most_common()[-20000:]

word_ind = [ i[0] for i in word_ind]

####### 딕셔너리 만드는 것이지만, 사용하지 않아 각주 처리
# {index : word } 
'''ind_to_dict ={}
for k,v in enumerate(word_ind):
  ind_to_dict[k] = v'''

# { word : index}
'''dict_to_ind = {}
for k,v in enumerate(word_ind):
  dict_to_ind[v] = k'''

#### 많이 안쓰는 단어가 몇개가 있는지 확인하기 위해, df_train.excerpt에서 단어 분리
splitword = df_train.excerpt.str.split(' ')

## 테스트용
##splitword_test = df_test.excerpt.str.split(' ')

# 책을 하나씩 읽어들여서 빈도수 낮은 단어의 갯수 추출
freq_excerpt_a=[]
for sw in splitword:
  cntless=0
  for j in sw:
    if j in word_ind:
      cntless+=1
  freq_excerpt_a.append(cntless)

## test용

'''freq_excerpt_test=[]
for sw in splitword_test:
  cntless=0
  for j in sw:
    if j in word_ind:
      cntless+=1
  freq_excerpt_test.append(cntless)'''

## 많이 안쓰는 단어 feature 추가 
df_train['freq_excerpt']=freq_excerpt_a

## test 용
##df_test['freq_excerpt']=freq_excerpt_test

#################################머신 러닝 실시

## 학습을 위해, x_train으로 변환

x_train=df_train.copy()

## test용
## x_test_set = df_test.copy()

## feature를 제외하고 나머지 칼럼 제거
x_train.drop(['id','url_legal','license','standard_error','target','excerpt'],axis=1, inplace=True)

## test용
## x_test_set.drop(['id','url_legal','license','excerpt'],axis=1, inplace=True)

### 레이블 y_train 제작
y_train = x_train.target

## 아까만든, 구간별 가중치 feature 추가 
x_train['level_score'] = totalscore

## train, test셋 구분
x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.3, random_state=777)

#### 모델 학습

RandomForest_Regressor = RandomForestRegressor()
RandomForest_Regressor.fit(x_train, y_train)

#### train 셋 학습 결과
print('tain_set학습결과:', RandomForest_Regressor.score(x_train, y_train))

#### test셋 학습 결과 
print('test_set학습결과:',RandomForest_Regressor.score(x_test, y_test))

##### 모델 저장
saved_model = pickle.dumps(RandomForest_Regressor)

joblib.dump(RandomForest_Regressor, 'bigmodel.pkl')

#### 모델 불러오기 및 예측 실시
'''
clf_from_joblib = joblib.load('SABON.pkl') 
clf_from_joblib.predict(x_test_set)'''